{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = '../../../'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load WebNLG dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jl254/miniconda3/envs/gat2/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"webnlg-challenge/web_nlg\"\n",
    "data_all = load_dataset(dataset_name, 'release_v3.0_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'Artist',\n",
       " 'size': 1,\n",
       " 'eid': 'Id988',\n",
       " 'original_triple_sets': {'otriple_set': [['Petah_Tikva | country | Israel']]},\n",
       " 'modified_triple_sets': {'mtriple_set': [['Petah_Tikva | country | Israel']]},\n",
       " 'shape': '(X (X))',\n",
       " 'shape_type': 'NA',\n",
       " 'lex': {'comment': ['', '', ''],\n",
       "  'lid': ['Id1', 'Id2', 'Id3'],\n",
       "  'text': ['Petah Tikva is a city in Israel.',\n",
       "   'Petah Tikva is in Israel.',\n",
       "   'Petah Tikva is in the country of Israel.'],\n",
       "  'lang': ['', '', '']},\n",
       " 'test_category': 'rdf-to-text-generation-test-data-with-refs-en',\n",
       " 'dbpedia_links': [],\n",
       " 'links': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all['test'][987]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13211/13211 [00:01<00:00, 8183.97it/s]\n",
      "100%|██████████| 1667/1667 [00:00<00:00, 8390.71it/s]\n",
      "100%|██████████| 5713/5713 [00:00<00:00, 8561.50it/s]\n"
     ]
    }
   ],
   "source": [
    "def extract(data):\n",
    "    \n",
    "    triple_set = set()\n",
    "    concept_set = set()\n",
    "    relation_set = set()\n",
    "    \n",
    "    for i in tqdm(range(len(data))):\n",
    "        entry = data[i]\n",
    "        for row in entry['modified_triple_sets']['mtriple_set']:\n",
    "            for triple in row:\n",
    "                # split triple by '|' and strip\n",
    "                triple = tuple([x.strip() for x in triple.split('|')])\n",
    "                triple_set.add(triple)\n",
    "                concept_set.update([triple[0], triple[2]])\n",
    "                relation_set.add(triple[1])\n",
    "        \n",
    "    return triple_set, concept_set, relation_set\n",
    "\n",
    "\n",
    "triple_set_train, concept_set_train, relation_set_train = extract(data_all['train'])\n",
    "triple_set_dev, concept_set_dev, relation_set_dev = extract(data_all['dev'])\n",
    "triple_set_test, concept_set_test, relation_set_test = extract(data_all['test'])\n",
    "\n",
    "# remove relations in test set that are not in train set\n",
    "relation_set_test = relation_set_test.intersection(relation_set_train)\n",
    "\n",
    "# remove triples in test set that have relations not in train set\n",
    "triple_set_test = set([x for x in triple_set_test if x[1] in relation_set_train])\n",
    "\n",
    "# save concept.txt, relation.txt\n",
    "\n",
    "concept_set = concept_set_train.union(concept_set_dev).union(concept_set_test)\n",
    "relation_set = relation_set_train.union(relation_set_dev).union(relation_set_test)\n",
    "\n",
    "\n",
    "# only keep top 20 most frequent relations\n",
    "relation_freq = defaultdict(int)\n",
    "for triple in triple_set_train.union(triple_set_dev).union(triple_set_test):\n",
    "    relation_freq[triple[1]] += 1\n",
    "relation_freq = sorted(relation_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "relation_freq = relation_freq[:]\n",
    "relation_set = set([x[0] for x in relation_freq])\n",
    "\n",
    "with open(os.path.join(repo_root, 'data/dbpedia/concept.txt'), 'w') as f:\n",
    "    for concept in concept_set:\n",
    "        f.write(concept + '\\n')\n",
    "\n",
    "with open(os.path.join(repo_root, 'data/dbpedia/relation.txt'), 'w') as f:\n",
    "    for relation in relation_set:\n",
    "        f.write(relation + '\\n')\n",
    "\n",
    "triple_set_test = set([x for x in triple_set_test if x[1] in relation_set])\n",
    "triple_set_dev = set([x for x in triple_set_dev if x[1] in relation_set])\n",
    "triple_set_train = set([x for x in triple_set_train if x[1] in relation_set])\n",
    "\n",
    "\n",
    "\n",
    "# # remove data in test that the relation is not in relation_set\n",
    "# data_all_new = {'train': [], 'dev': [], 'test': []}\n",
    "\n",
    "# for frame in ['train', 'dev', 'test']:\n",
    "#     data_ori = data_all[frame]\n",
    "#     for i in tqdm(range(len(data_ori))):\n",
    "#         entry = data_ori[i]\n",
    "#         # obtain the triples\n",
    "#         triples = entry['modified_triple_sets']['mtriple_set']\n",
    "#         triples = [[tuple([x.strip() for x in triple.split('|')]) for triple in row] for row in triples]\n",
    "#         # if any of the triples' relation is not in relation_set, skip, otherwise add to new data\n",
    "#         if any([triple[1] not in relation_set for row in triples for triple in row]):\n",
    "#             continue\n",
    "#         data_all_new[frame].append(entry)\n",
    "\n",
    "# data_all = data_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13211/13211 [00:01<00:00, 8258.26it/s]\n",
      "100%|██████████| 1667/1667 [00:00<00:00, 8657.17it/s]\n",
      "100%|██████████| 5713/5713 [00:00<00:00, 8952.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# data_all_new = {'train': [], 'dev': [], 'test': []}\n",
    "\n",
    "# for frame in ['train', 'dev', 'test']:\n",
    "#     data_ori = data_all[frame]\n",
    "#     for i in tqdm(range(len(data_ori))):\n",
    "#         entry = data_ori[i]\n",
    "#         # obtain the triples\n",
    "#         triples = entry['modified_triple_sets']['mtriple_set']\n",
    "#         # replace relation with new relation\n",
    "#         triples_new = []\n",
    "#         SKIP = False\n",
    "#         for row in triples:\n",
    "#             for triple in row:\n",
    "#                 # split triple by '|' and strip\n",
    "#                 triple = tuple([x.strip() for x in triple.split('|')])\n",
    "#                 if triple[1] not in relation_mapping:\n",
    "#                     SKIP = True\n",
    "#                     break\n",
    "#                 if triple[1] in relation_mapping:\n",
    "#                     triple = [triple[0], relation_mapping[triple[1]], triple[2]]\n",
    "#                     triple = triple[0] + ' | ' + triple[1] + ' | ' + triple[2]\n",
    "#                     triples_new.append(triple)\n",
    "        \n",
    "#         if SKIP:\n",
    "#             continue\n",
    "#         triples = triples_new\n",
    "\n",
    "#         entry['modified_triple_sets']['mtriple_set'] = triples\n",
    "#         data_all_new[frame].append(entry)\n",
    "\n",
    "# # data_all = data_all_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ardmore_Airport_(New_Zealand) | TransportationCharacteristics | \"07/25\"']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all['train'][121]['modified_triple_sets']['mtriple_set']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation by Merging different graph-text pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def combine_samples_once(dataset, min_size=6):\n",
    "    combined_triples = []\n",
    "    combined_text = []\n",
    "    current_size = 0\n",
    "\n",
    "    dataset = dataset.shuffle()\n",
    "    \n",
    "    for sample in dataset:\n",
    "        # Extract all triples from the sample\n",
    "        triples = sample['modified_triple_sets']['mtriple_set']\n",
    "        text = random.choice(sample['lex']['text'])  # Randomly sample a text\n",
    "        size = sample['size']\n",
    "\n",
    "        SKIP = False\n",
    "        for row in triples:\n",
    "            for triple in row:\n",
    "                triple = tuple([x.strip() for x in triple.split('|')])\n",
    "                rel = triple[1]\n",
    "                if rel not in relation_set:\n",
    "                    SKIP = True\n",
    "                    break\n",
    "        \n",
    "        if SKIP:\n",
    "            continue\n",
    "\n",
    "        # Add all triples to the combined results\n",
    "        combined_triples.extend(triples)\n",
    "        combined_text.append(text)\n",
    "        current_size += size\n",
    "\n",
    "        # If the combined size is greater than or equal to the threshold, stop combining\n",
    "        if current_size >= min_size:\n",
    "            break\n",
    "        \n",
    "    # convert combined triples [[], [], []] to []\n",
    "    combined_triples = [item for sublist in combined_triples for item in sublist]\n",
    "    # Return the combined results\n",
    "    return {\n",
    "        'combined_triples': tuple(combined_triples),  # Use tuple to ensure hashability\n",
    "        'combined_text': \" \".join(combined_text),\n",
    "        'combined_size': current_size\n",
    "    }\n",
    "\n",
    "def generate_unique_combinations(dataset, num_combinations=10000, min_size=10):\n",
    "    unique_combinations = set()\n",
    "    results = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(total=num_combinations) as pbar:\n",
    "        # Keep generating until we reach the required number of unique combinations\n",
    "        while len(unique_combinations) < num_combinations:\n",
    "            try:\n",
    "                combined_sample = combine_samples_once(dataset, min_size)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # Sort the combined triples to avoid duplicates due to different order\n",
    "            sorted_triples = tuple(sorted(combined_sample['combined_triples']))\n",
    "            \n",
    "            # Ensure the sorted combination of triples is unique\n",
    "            if sorted_triples not in unique_combinations:\n",
    "                unique_combinations.add(sorted_triples)\n",
    "                combined_sample['combined_triples'] = sorted_triples  # Ensure the result keeps sorted triples\n",
    "                results.append(combined_sample)\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "# combined_sample = combine_samples_once(data_all['train'], min_size=10)\n",
    "\n",
    "def generate_new_data(split, num_combinations=10000, min_size=4):\n",
    "    unique_combinations = generate_unique_combinations(data_all[split], num_combinations=num_combinations, min_size=min_size)\n",
    "    # save the unique combinations\n",
    "    with open(os.path.join(repo_root, f'data/webnlg/webnlg_{split}_combinations.json'), 'w') as f:\n",
    "        json.dump(unique_combinations, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:20<00:00, 62.38it/s]\n",
      "100%|██████████| 2000/2000 [00:23<00:00, 83.52it/s]\n",
      "100%|██████████| 1000/1000 [00:36<00:00, 27.71it/s]\n"
     ]
    }
   ],
   "source": [
    "num_dict = {\n",
    "    'train': 5000,\n",
    "    'dev': 2000,\n",
    "    'test': 1000\n",
    "}\n",
    "\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    generate_new_data(split, num_combinations=num_dict[split], min_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [03:04<00:00, 27.11it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 238440.08it/s]\n",
      "100%|██████████| 2000/2000 [00:44<00:00, 44.62it/s] \n",
      "100%|██████████| 2000/2000 [00:00<00:00, 236518.68it/s]\n",
      "100%|██████████| 1000/1000 [01:11<00:00, 14.04it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 344983.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def mix_combinations(positive_samples, num_mixed_samples):\n",
    "    unique_mixed_samples = set()  # To store unique mixed samples\n",
    "    mixed_results = []\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    with tqdm(total=num_mixed_samples) as pbar:\n",
    "        while len(unique_mixed_samples) < num_mixed_samples:\n",
    "            # Step 1: Randomly select two different positive samples\n",
    "            sample_a = random.choice(positive_samples)\n",
    "            sample_b = random.choice(positive_samples)\n",
    "            \n",
    "            # Ensure we are not mixing the same sample\n",
    "            if sample_a == sample_b:\n",
    "                continue\n",
    "            \n",
    "            # Step 2: Create a new mixed sample (a1, b2) - a1 (triples), b2 (text)\n",
    "            mixed_triples = sample_a['combined_triples']\n",
    "            mixed_text = sample_b['combined_text']\n",
    "            \n",
    "            # Step 3: Sort the combined triples to avoid duplicates due to different order\n",
    "            sorted_triples = tuple(sorted(mixed_triples))\n",
    "\n",
    "            # Step 4: Ensure the mixed sample is unique\n",
    "            if (sorted_triples, mixed_text) not in unique_mixed_samples:\n",
    "                # Add to the set of unique samples\n",
    "                unique_mixed_samples.add((sorted_triples, mixed_text))\n",
    "\n",
    "                # Add the mixed sample to results\n",
    "                mixed_results.append({\n",
    "                    'combined_triples': sorted_triples,\n",
    "                    'combined_text': mixed_text,\n",
    "                    'combined_size': len(sorted_triples)\n",
    "                })\n",
    "                \n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "    return mixed_results\n",
    "\n",
    "\n",
    "def generate_negative_data(split, num_combinations=10000, min_size=8):\n",
    "    positive = generate_unique_combinations(data_all[split], num_combinations=num_combinations, min_size=min_size)\n",
    "    negative = mix_combinations(positive, num_mixed_samples=num_combinations)\n",
    "\n",
    "    # save the unique combinations\n",
    "    with open(os.path.join(repo_root, f'data/webnlg/webnlg_{split}_negative.json'), 'w') as f:\n",
    "        json.dump(negative, f)\n",
    "\n",
    "\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    generate_negative_data(split, num_combinations=num_dict[split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 684292.75it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 70021.07it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 660936.65it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 651592.98it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 559912.43it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 972705.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare `statement` data following CommonsenseQA, OpenBookQA\n",
    "webnlg_root = f'{repo_root}/data/webnlg'\n",
    "os.system(f'mkdir -p {webnlg_root}/statement')\n",
    "\n",
    "for fname in [\"train\", 'dev', \"test\"]:\n",
    "    # read repo_root/data/webnlg/webnlg_{frame}_combinations.json as positive\n",
    "    with open(f'{webnlg_root}/webnlg_{fname}_combinations.json') as f:\n",
    "        positive = json.load(f)\n",
    "    with open(f'{webnlg_root}/webnlg_{fname}_negative.json') as f:\n",
    "        negative = json.load(f)\n",
    "    \n",
    "    def process_data(data, label=0):\n",
    "        examples = []\n",
    "        \n",
    "        for i in tqdm(range(len(data))):\n",
    "            line = data[i]\n",
    "            _id  = f\"{fname}-{i:05d}-{label}\"\n",
    "\n",
    "            # if label is 0, then answerKey is A, otherwise B\n",
    "            answerKey = 'A' if label == 0 else 'B'\n",
    "            stem = line['combined_text']\n",
    "            triples = line['combined_triples']\n",
    "            stmts = stem\n",
    "            ex_obj    = {\"id\": _id, \n",
    "                        \"question\": {\"stem\": stem, \"choices\": [{'text': \"\"}], 'triples': triples}, \n",
    "                        \"answerKey\": answerKey, \n",
    "                        \"statements\": stmts\n",
    "                        }\n",
    "            examples.append(ex_obj)\n",
    "        \n",
    "        return examples\n",
    "\n",
    "    pos_examples = process_data(positive, label=1)\n",
    "    neg_examples = process_data(negative, label=0)\n",
    "\n",
    "    # combine positive and negative examples\n",
    "    all_examples = pos_examples + neg_examples\n",
    "    random.shuffle(all_examples)\n",
    "    \n",
    "    with open(f'{webnlg_root}/statement/{fname}.statement.jsonl', 'w') as fout:\n",
    "        for dic in all_examples:\n",
    "            print(json.dumps(dic), file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Statements\n",
    "Only keep more common relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "# read concept.txt, relation.txt\n",
    "with open(os.path.join(repo_root, 'data/dbpedia/concept.txt'), 'r') as f:\n",
    "    concept_list = f.read().splitlines()\n",
    "\n",
    "with open(os.path.join(repo_root, 'data/dbpedia/relation.txt'), 'r') as f:\n",
    "    relation_list = f.read().splitlines()\n",
    "\n",
    "id2concept = concept_list\n",
    "id2relation = relation_list\n",
    "\n",
    "concept2id = {concept: i for i, concept in enumerate(concept_list)}\n",
    "relation2id = {relation: i for i, relation in enumerate(relation_list)}\n",
    "\n",
    "def construct_graph(triple_set):\n",
    "    graph = nx.MultiDiGraph()\n",
    "    attrs = set()\n",
    "    \n",
    "    for triple in triple_set:\n",
    "        subj = concept2id[triple[0]]\n",
    "        obj = concept2id[triple[2]]\n",
    "        rel = relation2id[triple[1]]\n",
    "        weight = 1.\n",
    "        graph.add_edge(subj, obj, rel=rel, weight=weight)\n",
    "        attrs.add((subj, obj, rel))\n",
    "        graph.add_edge(obj, subj, rel=rel + len(relation2id), weight=weight)\n",
    "        attrs.add((obj, subj, rel + len(relation2id)))\n",
    "\n",
    "    output_path = f\"{repo_root}/data/dbpedia/dbpedia.graph\"\n",
    "    nx.write_gpickle(graph, output_path)\n",
    "    \n",
    "    return graph\n",
    "\n",
    "\n",
    "KG = construct_graph(triple_set_train.union(triple_set_dev).union(triple_set_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each example in test set, check \n",
    "# with open(f'{webnlg_root}/statement/test.statement_ori.jsonl') as f:\n",
    "#     data = [json.loads(line) for line in f]\n",
    "\n",
    "# print(len(data))\n",
    "\n",
    "# def check_statement_test(example):\n",
    "#     triples = example['question']['choices']\n",
    "#     for triple in triples:\n",
    "#         triple = tuple([x.strip() for x in triple.split('|')]) \n",
    "#         rel = triple[1]\n",
    "#         if rel not in relation_list:\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# data = [x for x in data if check_statement_test(x)]\n",
    "\n",
    "# print(len(data))\n",
    "\n",
    "# # save the filtered data\n",
    "# with open(f'{webnlg_root}/statement/test.statement.jsonl', 'w') as f:\n",
    "#     for dic in data:\n",
    "#         print(json.dumps(dic), file=f)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 52812.82it/s]\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 52799.05it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 48411.27it/s]\n"
     ]
    }
   ],
   "source": [
    "def process(frame):\n",
    "    with open (f'{webnlg_root}/statement/{frame}.statement.jsonl') as f:\n",
    "        stmts = [json.loads(line) for line in f]\n",
    "    with open(f\"{webnlg_root}/grounded/{frame}.grounded.jsonl\", 'w') as fout:\n",
    "        for stmt in tqdm(stmts):\n",
    "            sent = stmt['question']['stem']\n",
    "            qc = []\n",
    "            qc_names = []\n",
    "            triples = stmt['question']['triples']\n",
    "            # obtain the entity names, split triples by '|' and strip, choose the first and last element\n",
    "            for triple in triples:\n",
    "                triple = [x.strip() for x in triple.split('|')]\n",
    "                qc_names.extend([triple[0], triple[2]])\n",
    "                qc.extend([concept2id[triple[0]], concept2id[triple[2]]])\n",
    "            \n",
    "            ans = stmt['answerKey']\n",
    "            out = {'sent': sent, 'ans': ans, 'qc': qc, 'qc_names': qc_names, 'ac': [], 'ac_names': [], 'triples': triples}\n",
    "            print (json.dumps(out), file=fout)\n",
    "\n",
    "\n",
    "os.system(f'mkdir -p {webnlg_root}/grounded')\n",
    "for frame in ['train', 'dev', 'test']:\n",
    "    process(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get KG subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kg():\n",
    "    global cpnet, cpnet_simple\n",
    "    cpnet = KG\n",
    "    cpnet_simple = nx.Graph()\n",
    "    for u, v, data in cpnet.edges(data=True):\n",
    "        w = data['weight'] if 'weight' in data else 1.0\n",
    "        if cpnet_simple.has_edge(u, v):\n",
    "            cpnet_simple[u][v]['weight'] += w\n",
    "        else:\n",
    "            cpnet_simple.add_edge(u, v, weight=w)\n",
    "\n",
    "load_kg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, coo_matrix\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def concepts2adj(node_ids):\n",
    "    global id2relation\n",
    "    cids = np.array(node_ids, dtype=np.int32)\n",
    "    n_rel = len(id2relation)\n",
    "    n_node = cids.shape[0]\n",
    "    adj = np.zeros((n_rel, n_node, n_node), dtype=np.uint8)\n",
    "    for s in range(n_node):\n",
    "        for t in range(n_node):\n",
    "            s_c, t_c = cids[s], cids[t]\n",
    "            if cpnet.has_edge(s_c, t_c):\n",
    "                for e_attr in cpnet[s_c][t_c].values():\n",
    "                    if e_attr['rel'] >= 0 and e_attr['rel'] < n_rel:\n",
    "                        adj[e_attr['rel']][s][t] = 1\n",
    "    adj = coo_matrix(adj.reshape(-1, n_node))\n",
    "    return adj, cids\n",
    "\n",
    "def concepts_to_adj_matrices_all_pair(data):\n",
    "    qc_ids, ac_ids = data\n",
    "    qa_nodes = set(qc_ids) | set(ac_ids)\n",
    "    schema_graph = sorted(qc_ids) + sorted(ac_ids)\n",
    "    arange = np.arange(len(schema_graph))\n",
    "    qmask = arange < len(qc_ids)\n",
    "    amask = (arange >= len(qc_ids)) & (arange < (len(qc_ids) + len(ac_ids)))\n",
    "    adj, concepts = concepts2adj(schema_graph)\n",
    "    return {'adj': adj, 'concepts': concepts, 'qmask': qmask, 'amask': amask, 'cid2score': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adj_data_from_grounded_concepts(grounded_path, cpnet_graph_path, cpnet_vocab_path, output_path, num_processes):\n",
    "    qa_data = []\n",
    "    with open(grounded_path, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            dic = json.loads(line)\n",
    "            q_ids = set(concept2id[c] for c in dic['qc_names'])\n",
    "            if not q_ids:\n",
    "                q_ids = {concept2id['31770']} \n",
    "            a_ids = set(concept2id[c] for c in dic['ac_names'])\n",
    "            if not a_ids:\n",
    "                a_ids = {concept2id['325']}\n",
    "            q_ids = q_ids - a_ids\n",
    "            qa_data.append((q_ids, a_ids))\n",
    "    \n",
    "    with Pool(num_processes) as p:\n",
    "        res = list(tqdm(p.imap(concepts_to_adj_matrices_all_pair, qa_data), total=len(qa_data)))\n",
    "    \n",
    "    lens = [len(e['concepts']) for e in res]\n",
    "    print ('mean #nodes', int(np.mean(lens)), 'med', int(np.median(lens)), '5th', int(np.percentile(lens, 5)), '95th', int(np.percentile(lens, 95)))\n",
    "\n",
    "    with open(output_path, 'wb') as fout:\n",
    "        pickle.dump(res, fout)\n",
    "\n",
    "    print(f'adj data saved to {output_path}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 14600.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean #nodes 10 med 11 5th 6 95th 16\n",
      "adj data saved to ../../..//data/webnlg/graph/train.graph.adj.pk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 12190.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean #nodes 10 med 11 5th 6 95th 16\n",
      "adj data saved to ../../..//data/webnlg/graph/dev.graph.adj.pk\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 14864.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean #nodes 10 med 11 5th 6 95th 15\n",
      "adj data saved to ../../..//data/webnlg/graph/test.graph.adj.pk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.system(f'mkdir -p {repo_root}/data/webnlg/graph')\n",
    "\n",
    "for fname in ['train', 'dev', \"test\"]:\n",
    "    grounded_path = f\"{repo_root}/data/webnlg/grounded/{fname}.grounded.jsonl\"\n",
    "    kg_path       = f\"{repo_root}/data/dbpedia/dbpedia.graph\"\n",
    "    kg_vocab_path = f\"{repo_root}/data/dbpedia/concept.txt\"\n",
    "    output_path   = f\"{repo_root}/data/webnlg/graph/{fname}.graph.adj.pk\"\n",
    "\n",
    "    generate_adj_data_from_grounded_concepts(grounded_path, kg_path, kg_vocab_path, output_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gat2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
